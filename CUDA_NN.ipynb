{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Basics"
      ],
      "metadata": {
        "id": "DCiiIzpKfa8-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D364-InsSjjw",
        "outputId": "c15a68e6-4940-4a11-be2b-54e4eb743cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFwVAcaocdGb",
        "outputId": "9877dd8c-bb68-44c5-b781-b2ec61846825"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmprduvcdeu\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello(){\n",
        "    printf(\"Hello from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "__host__ int main(){\n",
        "    hello<<<3, 3>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1tcZtpacjhV",
        "outputId": "9700f4ee-8ecb-4c7b-f1ef-ac40527450ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block: 1, thread: 0\n",
            "Hello from block: 1, thread: 1\n",
            "Hello from block: 1, thread: 2\n",
            "Hello from block: 2, thread: 0\n",
            "Hello from block: 2, thread: 1\n",
            "Hello from block: 2, thread: 2\n",
            "Hello from block: 0, thread: 0\n",
            "Hello from block: 0, thread: 1\n",
            "Hello from block: 0, thread: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello() {\n",
        "    printf(\"Hello from block: (%u, %u, %u), thread: (%u, %u, %u)\\n\",\n",
        "           blockIdx.x, blockIdx.y, blockIdx.z,\n",
        "           threadIdx.x, threadIdx.y, threadIdx.z);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the dimensions of the grid and blocks\n",
        "    dim3 gridDim(2, 2, 2);   // 2x2x2 grid of blocks\n",
        "    dim3 blockDim(2, 2, 2);  // 2x2x2 grid of threads per block\n",
        "\n",
        "    // Launch the kernel\n",
        "    hello<<<gridDim, blockDim>>>();\n",
        "\n",
        "    // Wait for GPU to finish before accessing on host\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm5frl3ZdhJ8",
        "outputId": "3354d15c-24da-4a28-dc96-ef8a9dbdf592"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block: (1, 0, 0), thread: (0, 0, 0)\n",
            "Hello from block: (1, 0, 0), thread: (1, 0, 0)\n",
            "Hello from block: (1, 0, 0), thread: (0, 1, 0)\n",
            "Hello from block: (1, 0, 0), thread: (1, 1, 0)\n",
            "Hello from block: (1, 0, 0), thread: (0, 0, 1)\n",
            "Hello from block: (1, 0, 0), thread: (1, 0, 1)\n",
            "Hello from block: (1, 0, 0), thread: (0, 1, 1)\n",
            "Hello from block: (1, 0, 0), thread: (1, 1, 1)\n",
            "Hello from block: (0, 1, 1), thread: (0, 0, 0)\n",
            "Hello from block: (0, 1, 1), thread: (1, 0, 0)\n",
            "Hello from block: (0, 1, 1), thread: (0, 1, 0)\n",
            "Hello from block: (0, 1, 1), thread: (1, 1, 0)\n",
            "Hello from block: (0, 1, 1), thread: (0, 0, 1)\n",
            "Hello from block: (0, 1, 1), thread: (1, 0, 1)\n",
            "Hello from block: (0, 1, 1), thread: (0, 1, 1)\n",
            "Hello from block: (0, 1, 1), thread: (1, 1, 1)\n",
            "Hello from block: (0, 0, 1), thread: (0, 0, 0)\n",
            "Hello from block: (0, 0, 1), thread: (1, 0, 0)\n",
            "Hello from block: (0, 0, 1), thread: (0, 1, 0)\n",
            "Hello from block: (0, 0, 1), thread: (1, 1, 0)\n",
            "Hello from block: (0, 0, 1), thread: (0, 0, 1)\n",
            "Hello from block: (0, 0, 1), thread: (1, 0, 1)\n",
            "Hello from block: (0, 0, 1), thread: (0, 1, 1)\n",
            "Hello from block: (0, 0, 1), thread: (1, 1, 1)\n",
            "Hello from block: (0, 1, 0), thread: (0, 0, 0)\n",
            "Hello from block: (0, 1, 0), thread: (1, 0, 0)\n",
            "Hello from block: (0, 1, 0), thread: (0, 1, 0)\n",
            "Hello from block: (0, 1, 0), thread: (1, 1, 0)\n",
            "Hello from block: (0, 1, 0), thread: (0, 0, 1)\n",
            "Hello from block: (0, 1, 0), thread: (1, 0, 1)\n",
            "Hello from block: (0, 1, 0), thread: (0, 1, 1)\n",
            "Hello from block: (0, 1, 0), thread: (1, 1, 1)\n",
            "Hello from block: (1, 1, 1), thread: (0, 0, 0)\n",
            "Hello from block: (1, 1, 1), thread: (1, 0, 0)\n",
            "Hello from block: (1, 1, 1), thread: (0, 1, 0)\n",
            "Hello from block: (1, 1, 1), thread: (1, 1, 0)\n",
            "Hello from block: (1, 1, 1), thread: (0, 0, 1)\n",
            "Hello from block: (1, 1, 1), thread: (1, 0, 1)\n",
            "Hello from block: (1, 1, 1), thread: (0, 1, 1)\n",
            "Hello from block: (1, 1, 1), thread: (1, 1, 1)\n",
            "Hello from block: (1, 1, 0), thread: (0, 0, 0)\n",
            "Hello from block: (1, 1, 0), thread: (1, 0, 0)\n",
            "Hello from block: (1, 1, 0), thread: (0, 1, 0)\n",
            "Hello from block: (1, 1, 0), thread: (1, 1, 0)\n",
            "Hello from block: (1, 1, 0), thread: (0, 0, 1)\n",
            "Hello from block: (1, 1, 0), thread: (1, 0, 1)\n",
            "Hello from block: (1, 1, 0), thread: (0, 1, 1)\n",
            "Hello from block: (1, 1, 0), thread: (1, 1, 1)\n",
            "Hello from block: (1, 0, 1), thread: (0, 0, 0)\n",
            "Hello from block: (1, 0, 1), thread: (1, 0, 0)\n",
            "Hello from block: (1, 0, 1), thread: (0, 1, 0)\n",
            "Hello from block: (1, 0, 1), thread: (1, 1, 0)\n",
            "Hello from block: (1, 0, 1), thread: (0, 0, 1)\n",
            "Hello from block: (1, 0, 1), thread: (1, 0, 1)\n",
            "Hello from block: (1, 0, 1), thread: (0, 1, 1)\n",
            "Hello from block: (1, 0, 1), thread: (1, 1, 1)\n",
            "Hello from block: (0, 0, 0), thread: (0, 0, 0)\n",
            "Hello from block: (0, 0, 0), thread: (1, 0, 0)\n",
            "Hello from block: (0, 0, 0), thread: (0, 1, 0)\n",
            "Hello from block: (0, 0, 0), thread: (1, 1, 0)\n",
            "Hello from block: (0, 0, 0), thread: (0, 0, 1)\n",
            "Hello from block: (0, 0, 0), thread: (1, 0, 1)\n",
            "Hello from block: (0, 0, 0), thread: (0, 1, 1)\n",
            "Hello from block: (0, 0, 0), thread: (1, 1, 1)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <ctime>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "//brute force approach to finding which point\n",
        "void findClosestCPU(float3* points, int* indices, int count) {\n",
        "    // Base case, if there's 1 point don't do anything\n",
        "    if(count <=1) return;\n",
        "    // Loop through every point\n",
        "    for (int curPoint = 0; curPoint < count; curPoint++) {\n",
        "        // set as close to the largest float possible\n",
        "        float distToClosest = 3.4028238f ;\n",
        "        // See how far it is from every other point\n",
        "        for (int i = 0; i < count; i++) {\n",
        "            // Don't check distance to itself\n",
        "            if(i == curPoint) continue;\n",
        "            float dist_sqr = (points[curPoint].x - points[i].x) *\n",
        "                (points[curPoint].x - points[i].x) +\n",
        "                (points[curPoint].y - points[i].y) *\n",
        "                (points[curPoint].y - points[i].y) +\n",
        "                (points[curPoint].z - points[i].z) *\n",
        "                (points[curPoint].z - points[i].z);\n",
        "            if(dist_sqr < distToClosest) {\n",
        "                distToClosest = dist_sqr;\n",
        "                indices[curPoint] = i;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "\n",
        "    //defining parameters\n",
        "    const int count = 10000;\n",
        "    int* indexOfClosest = new int[count];\n",
        "    float3* points = new float3[count];\n",
        "\n",
        "    //defining random points\n",
        "    for (int i = 0; i < count; i++){\n",
        "        points[i].x = (float)(((rand()%10000))-5000);\n",
        "        points[i].y = (float)(((rand()%10000))-5000);\n",
        "        points[i].z = (float)(((rand()%10000))-5000);\n",
        "    }\n",
        "\n",
        "    long fastest = 1000000000;\n",
        "\n",
        "    cout << \"running brute force nearest neighbor on the CPU...\"<<endl;\n",
        "    for (int i = 0; i <= 10; i++){\n",
        "        long start = clock();\n",
        "        findClosestCPU(points, indexOfClosest, count);\n",
        "        double duration = ( clock() - start ) / (double) CLOCKS_PER_SEC;\n",
        "        cout << \"test \" << i << \" took \" << duration << \" seconds\" <<endl;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVt0SL6VfdSU",
        "outputId": "869e022d-22f2-419a-8f23-94ea0d258844"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running brute force nearest neighbor on the CPU...\n",
            "test 0 took 1.2005 seconds\n",
            "test 1 took 1.18997 seconds\n",
            "test 2 took 1.18641 seconds\n",
            "test 3 took 1.17282 seconds\n",
            "test 4 took 1.91539 seconds\n",
            "test 5 took 1.49837 seconds\n",
            "test 6 took 1.18263 seconds\n",
            "test 7 took 1.17336 seconds\n",
            "test 8 took 1.21073 seconds\n",
            "test 9 took 1.18849 seconds\n",
            "test 10 took 1.17642 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile findClosestGPU.cu\n",
        "#include <iostream>\n",
        "#include <ctime>\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Brute force implementation, parallelized on the GPU\n",
        "__global__ void findClosestGPU(float3* points, int* indices, int count) {\n",
        "    if (count <= 1) return;\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (idx < count) {\n",
        "        float3 thisPoint = points[idx];\n",
        "        float smallestSoFar = 3.40282e38f;\n",
        "\n",
        "        for (int i = 0; i < count; i++) {\n",
        "            if (i == idx) continue;\n",
        "\n",
        "            float dist_sqr = (thisPoint.x - points[i].x) *\n",
        "                             (thisPoint.x - points[i].x) +\n",
        "                             (thisPoint.y - points[i].y) *\n",
        "                             (thisPoint.y - points[i].y) +\n",
        "                             (thisPoint.z - points[i].z) *\n",
        "                             (thisPoint.z - points[i].z);\n",
        "            if (dist_sqr < smallestSoFar) {\n",
        "                smallestSoFar = dist_sqr;\n",
        "                indices[idx] = i;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Defining parameters\n",
        "    const int count = 10000;\n",
        "    int* h_indexOfClosest = new int[count];\n",
        "    float3* h_points = new float3[count];\n",
        "\n",
        "    // Defining random points\n",
        "    for (int i = 0; i < count; i++) {\n",
        "        h_points[i].x = (float)(((rand() % 10000)) - 5000);\n",
        "        h_points[i].y = (float)(((rand() % 10000)) - 5000);\n",
        "        h_points[i].z = (float)(((rand() % 10000)) - 5000);\n",
        "    }\n",
        "\n",
        "    // Device pointers\n",
        "    int* d_indexOfClosest;\n",
        "    float3* d_points;\n",
        "\n",
        "    // Allocating memory on the device\n",
        "    cudaMalloc(&d_indexOfClosest, sizeof(int) * count);\n",
        "    cudaMalloc(&d_points, sizeof(float3) * count);\n",
        "\n",
        "    // Copying values from the host to the device\n",
        "    cudaMemcpy(d_points, h_points, sizeof(float3) * count, cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threads_per_block = 64;\n",
        "    cout << \"Running brute force nearest neighbor on the GPU...\" << endl;\n",
        "    for (int i = 1; i <= 10; i++) {\n",
        "        long start = clock();\n",
        "\n",
        "        findClosestGPU<<<(count / threads_per_block) + 1, threads_per_block>>>(d_points, d_indexOfClosest, count);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Copying results from the device to the host\n",
        "        cudaMemcpy(h_indexOfClosest, d_indexOfClosest, sizeof(int) * count, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        double duration = (clock() - start) / (double)CLOCKS_PER_SEC;\n",
        "        cout << \"Test \" << i << \" took \" << duration << \" seconds\" << endl;\n",
        "    }\n",
        "\n",
        "    // Freeing device memory\n",
        "    cudaFree(d_indexOfClosest);\n",
        "    cudaFree(d_points);\n",
        "\n",
        "    // Freeing host memory\n",
        "    delete[] h_indexOfClosest;\n",
        "    delete[] h_points;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzEA2Hfofqof",
        "outputId": "090ee514-93ad-4c7a-84c8-6845f259be95"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing findClosestGPU.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./findClosestGPU.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fJXUySCgm-u",
        "outputId": "0557fdad-0d6e-446f-f4ae-3c6beb8a8d79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==5284== NVPROF is profiling process 5284, command: ./findClosestGPU.out\n",
            "Running brute force nearest neighbor on the GPU...\n",
            "Test 1 took 0.00267 seconds\n",
            "Test 2 took 0.002464 seconds\n",
            "Test 3 took 0.002435 seconds\n",
            "Test 4 took 0.002429 seconds\n",
            "Test 5 took 0.002435 seconds\n",
            "Test 6 took 0.002429 seconds\n",
            "Test 7 took 0.002428 seconds\n",
            "Test 8 took 0.00243 seconds\n",
            "Test 9 took 0.002434 seconds\n",
            "Test 10 took 0.002437 seconds\n",
            "==5284== Profiling application: ./findClosestGPU.out\n",
            "==5284== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   99.76%  23.830ms        10  2.3830ms  2.3798ms  2.3845ms  findClosestGPU(float3*, int*, int)\n",
            "                    0.19%  45.760us        10  4.5760us  4.4480us  5.0240us  [CUDA memcpy DtoH]\n",
            "                    0.05%  12.448us         1  12.448us  12.448us  12.448us  [CUDA memcpy HtoD]\n",
            "      API calls:   79.38%  95.898ms         2  47.949ms  4.5110us  95.893ms  cudaMalloc\n",
            "                   19.76%  23.871ms        10  2.3871ms  2.3834ms  2.3915ms  cudaDeviceSynchronize\n",
            "                    0.31%  372.80us        11  33.890us  26.631us  55.615us  cudaMemcpy\n",
            "                    0.29%  347.97us        10  34.797us  11.344us  216.42us  cudaLaunchKernel\n",
            "                    0.14%  165.98us       114  1.4550us     159ns  61.714us  cuDeviceGetAttribute\n",
            "                    0.10%  122.50us         2  61.248us  12.087us  110.41us  cudaFree\n",
            "                    0.01%  11.587us         1  11.587us  11.587us  11.587us  cuDeviceGetName\n",
            "                    0.00%  5.2270us         1  5.2270us  5.2270us  5.2270us  cuDeviceGetPCIBusId\n",
            "                    0.00%  4.3750us         1  4.3750us  4.3750us  4.3750us  cuDeviceTotalMem\n",
            "                    0.00%  1.4590us         3     486ns     203ns     940ns  cuDeviceGetCount\n",
            "                    0.00%     861ns         2     430ns     170ns     691ns  cuDeviceGet\n",
            "                    0.00%     818ns         1     818ns     818ns     818ns  cuModuleGetLoadingMode\n",
            "                    0.00%     315ns         1     315ns     315ns     315ns  cuDeviceGetUuid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building NN from Scratch"
      ],
      "metadata": {
        "id": "TicWyZX6A1G8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preliminaries of C"
      ],
      "metadata": {
        "id": "nbK0zfBHA5w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile someClass.hh\n",
        "\n",
        "// this is used so, if someClass gets imported multiple times across several\n",
        "// documents, it only actually gets imported once.\n",
        "#pragma once\n",
        "\n",
        "class ClassWithFunctionality {\n",
        "    // defining private things for internal use\n",
        "private:\n",
        "    // defining private data\n",
        "    int someValue;\n",
        "    int anotherValue;\n",
        "\n",
        "    // defining private functions\n",
        "    void privateFunction1();\n",
        "    void privateFunction2();\n",
        "\n",
        "    // defining things accessible outside the object\n",
        "public:\n",
        "    // defining public data\n",
        "    int somePublicValue;\n",
        "    int someOtherPublicValue;\n",
        "\n",
        "    // defining public functions\n",
        "    ClassWithFunctionality(int constructorInput);\n",
        "    void doSomething1();\n",
        "    void doSomething2();\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIOWQRM7A3h2",
        "outputId": "1592ea86-372f-4c1c-f430-bc4af0dbe093"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing someClass.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile someClass.cu\n",
        "\n",
        "#include \"someClass.hh\"\n",
        "\n",
        "ClassWithFunctionality::ClassWithFunctionality(int constructorInput)\n",
        "    : someValue(constructorInput), anotherValue(2), somePublicValue(3), someOtherPublicValue(4)\n",
        "{}\n",
        "\n",
        "void ClassWithFunctionality::doSomething1() {\n",
        "    return;\n",
        "}\n",
        "\n",
        "void ClassWithFunctionality::doSomething2() {\n",
        "    return;\n",
        "}\n",
        "\n",
        "void ClassWithFunctionality::privateFunction1() {\n",
        "    return;\n",
        "}\n",
        "\n",
        "void ClassWithFunctionality::privateFunction2() {\n",
        "    return;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W14uwOLcBgMJ",
        "outputId": "01915469-1e41-433a-a2b3-595e2d35113c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing someClass.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include \"someClass.hh\"\n",
        "\n",
        "// testing SomeClass\n",
        "int main(void) {\n",
        "    ClassWithFunctionality example(3);\n",
        "    std::cout << \"it works!\" << std::endl;\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqronaMhB3VP",
        "outputId": "2cba4d6b-36f2-4d77-ef70-b75eabf89d07"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "nvcc someClass.cu main.cu -o main\n",
        "./main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN0Xl4O1B673",
        "outputId": "438195ea-303f-40ed-c960-68a9926e1f65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it works!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##main"
      ],
      "metadata": {
        "id": "nMvkX8qTA7hc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###File: Shape\n",
        "\n",
        "This is for matrices with a 2D shape"
      ],
      "metadata": {
        "id": "bAe1qbIdDFoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile shape.hh\n",
        "\n",
        "#pragma once\n",
        "\n",
        "struct Shape {\n",
        "  size_t x, y;\n",
        "\n",
        "  Shape(size_t x = 1, size_t y = 1);\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yh4JP0TA8pl",
        "outputId": "476fb765-5454-4ddd-b922-8ff03b41d1b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing shape.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile shape.cu\n",
        "#include \"shape.hh\"\n",
        "\n",
        "Shape::Shape(size_t x, size_t y) :\n",
        " x(x), y(y)\n",
        "{ }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_d7fIjEDZEw",
        "outputId": "aad118b5-e4b0-437d-b01a-7c177cec606f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing shape.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"shape.hh\"\n",
        "#include <iostream>\n",
        "#include <stdio.h>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "int main( void ) {\n",
        "  Shape shape = Shape(100, 200);\n",
        "  cout << \"x: \" << shape.x << \" y: \" << shape.y << endl;\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIjMlJCeDjSW",
        "outputId": "630ec344-a782-47f0-c630-51b6b573ad4e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc shape.cu main.cu -o shape.out\n",
        "./shape.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZI_YjSgD_Id",
        "outputId": "a01c6836-5864-4498-d0fb-30758f4a43da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: 100 y: 200\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###File: NNException\n",
        "\n",
        "NNException is a lightweight wrapper built around cudaGetLastError that allows us to check, throughout our code, if there was an error on the GPU"
      ],
      "metadata": {
        "id": "7GJ5kIiYFRjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nn_exception.hh\n",
        "#pragma once\n",
        "\n",
        "#include <exception>\n",
        "#include <iostream>\n",
        "\n",
        "class NNException : std::exception {\n",
        "  private:\n",
        "    const char* exception_message;\n",
        "\n",
        "  public:\n",
        "    NNException(const char* exception_message) :\n",
        "      exception_message(exception_message)\n",
        "    { }\n",
        "\n",
        "    virtual const char* what() const throw()\n",
        "    {\n",
        "      return exception_message;\n",
        "    }\n",
        "\n",
        "    static void throwIfDeviceErrorsOccurred(const char* exception_message){\n",
        "      cudaError_t error = cudaGetLastError();\n",
        "      if (error != cudaSuccess) {\n",
        "        throw NNException(exception_message);\n",
        "      }\n",
        "    }\n",
        "};\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUd3LTuJFcsH",
        "outputId": "282c0ca6-34a3-47cd-ce44-6d1de31f5c7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing nn_exception.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "\n",
        "#include \"nn_exception.hh\"\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "int main() {\n",
        "  float* d_data;\n",
        "  cudaError_t error = cudaMalloc((void**)&d_data, 100*sizeof(float)); // allocate memory for the device = GPU\n",
        "\n",
        "  try {\n",
        "    NNException::throwIfDeviceErrorsOccurred(\"Failed to allocate GPU memry\");\n",
        "  }\n",
        "  catch (const NNException& e) {\n",
        "    std::cerr << e.what() << std::endl;\n",
        "    return -1;\n",
        "  }\n",
        "  error = cudaFree(d_data);\n",
        "\n",
        "  // Check for CUDA errors again\n",
        "    try {\n",
        "        NNException::throwIfDeviceErrorsOccurred(\"Failed to free GPU memory\");\n",
        "    } catch (const NNException& e) {\n",
        "        std::cerr << \"Caught NNException: \" << e.what() << std::endl;\n",
        "        return -1; // Return an error code\n",
        "    }\n",
        "\n",
        "    std::cout << \"CUDA operations completed successfully\" << std::endl;\n",
        "    return 0; // Return success\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o7M7JEEGqcG",
        "outputId": "1abc25f9-7e5e-4e31-c520-5ec807132a28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvcc main.cu shape.cu -o nnexception.out\n",
        "./nnexception.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ahp-A0GRHnxn",
        "outputId": "dd632a95-1a93-4f19-caaa-0517729cb8d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA operations completed successfully\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###File: Matrix\n",
        "\n",
        "This class abstracts some of the communication between the device and host, allowing a matrix of values to easily be passed between memory locations. It allows for:\n",
        "\n",
        "* memory to be allocated on the GPU for the matrix\n",
        "* memory to be allocated on the CPU for the matrix\n",
        "* memory to be allocated on both the CPU and GPU for the matrix\n",
        "allocate memory, if it isn’t allocated already\n",
        "* copy data from the CPU RAM to GPU VRAM\n",
        "* copy data from the GPU VRAM to CPU RAM\n",
        "* overrides to allow the matrix to be indexed like an array"
      ],
      "metadata": {
        "id": "VtX-zM49FdUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix.hh\n",
        "\n",
        "#pragma once\n",
        "\n",
        "#include \"shape.hh\"\n",
        "#include <memory>\n",
        "\n",
        "class Matrix {\n",
        "  private:\n",
        "    bool device_allocated;\n",
        "    bool host_allocated;\n",
        "\n",
        "    void allocateCudaMemory();\n",
        "    void allocateHostMemory();\n",
        "\n",
        "  public:\n",
        "    Shape shape;\n",
        "    std::shared_ptr<float> data_device;\n",
        "    std::shared_ptr<float> data_host;\n",
        "\n",
        "    Matrix(size_t x_dim = 1, size_t y_dim = 1);\n",
        "    Matrix(Shape shape);\n",
        "\n",
        "    void allocateMemory();\n",
        "    void allocateMemoryIfNotAllocated(Shape shape);\n",
        "\n",
        "    void copyHostToDevice();\n",
        "    void copyDeviceToHost();\n",
        "\n",
        "    float& operator[](const int index);\n",
        "    const float& operator[](const int index) const;\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFtHpsmjIr5g",
        "outputId": "b0181258-4fe6-4f23-aeed-f3f490a3232b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matrix.cu\n",
        "\n",
        "#include \"matrix.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "Matrix::Matrix(size_t x_dim, size_t y_dim) :\n",
        "  shape(x_dim, y_dim),\n",
        "  data_device(nullptr),\n",
        "  data_host(nullptr),\n",
        "  device_allocated(false),\n",
        "  host_allocated(false)\n",
        "{ }\n",
        "\n",
        "Matrix::Matrix(Shape shape) :\n",
        "  Matrix(shape.x, shape.y)\n",
        "{ }\n",
        "\n",
        "void Matrix::allocateCudaMemory() {\n",
        "  if (!device_allocated) {\n",
        "    float* device_memory = nullptr;\n",
        "    cudaMalloc(&device_memory, shape.x * shape.y * sizeof(float));\n",
        "\n",
        "    NNException::throwIfDeviceErrorsOccurred(\"Cannot allocate CUDA memory for tensor3D\");\n",
        "    data_device = std::shared_ptr<float>(device_memory, [&](float* ptr){ cudaFree(ptr);});\n",
        "    device_allocated = true;\n",
        "  }\n",
        "}\n",
        "void Matrix::allocateHostMemory() {\n",
        " if (!host_allocated) {\n",
        "  data_host = std::shared_ptr<float>(new float[shape.x * shape.y],\n",
        "             [&](float* ptr){ delete[] ptr; });\n",
        "  host_allocated = true;\n",
        " }\n",
        "}\n",
        "\n",
        "void Matrix::allocateMemory() {\n",
        " allocateCudaMemory();\n",
        " allocateHostMemory();\n",
        "}\n",
        "\n",
        "void Matrix::allocateMemoryIfNotAllocated(Shape shape) {\n",
        " if (!device_allocated && !host_allocated) {\n",
        "  this->shape = shape;\n",
        "  allocateMemory();\n",
        " }\n",
        "}\n",
        "\n",
        "void Matrix::copyHostToDevice() {\n",
        " if (device_allocated && host_allocated) {\n",
        "  cudaMemcpy(data_device.get(), data_host.get(), shape.x * shape.y * sizeof(float), cudaMemcpyHostToDevice);\n",
        "  NNException::throwIfDeviceErrorsOccurred(\"Cannot copy host data to CUDA device.\");\n",
        " }\n",
        " else {\n",
        "  throw NNException(\"Cannot copy host data to not allocated memory on device.\");\n",
        " }\n",
        "}\n",
        "\n",
        "void Matrix::copyDeviceToHost() {\n",
        " if (device_allocated && host_allocated) {\n",
        "  cudaMemcpy(data_host.get(), data_device.get(), shape.x * shape.y * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "  NNException::throwIfDeviceErrorsOccurred(\"Cannot copy device data to host.\");\n",
        " }\n",
        " else {\n",
        "  throw NNException(\"Cannot copy device data to not allocated memory on host.\");\n",
        " }\n",
        "}\n",
        "\n",
        "float& Matrix::operator[](const int index) {\n",
        " return data_host.get()[index];\n",
        "}\n",
        "\n",
        "const float& Matrix::operator[](const int index) const {\n",
        " return data_host.get()[index];\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki_58pgsJ4X_",
        "outputId": "9aa08cec-500c-4b7c-9616-12f12a829f08"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing matrix.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include \"matrix.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "\n",
        "int main() {\n",
        "    // Create a Matrix object with dimensions 10x10\n",
        "    Matrix matrix(10, 10);\n",
        "\n",
        "    // Allocate memory on both host and device\n",
        "    matrix.allocateMemory();\n",
        "    std::cout << \"Memory allocated on host and device.\" << std::endl;\n",
        "\n",
        "    // Initialize host data\n",
        "    for (size_t i = 0; i < 100; ++i) {\n",
        "        matrix[i] = static_cast<float>(i);\n",
        "    }\n",
        "    std::cout << \"Host data initialized.\" << std::endl;\n",
        "\n",
        "    // Copy data from host to device\n",
        "    matrix.copyHostToDevice();\n",
        "    std::cout << \"Data copied from host to device.\" << std::endl;\n",
        "\n",
        "    // Clear host data\n",
        "    for (size_t i = 0; i < 100; ++i) {\n",
        "        matrix[i] = 0.0f;\n",
        "    }\n",
        "    std::cout << \"Host data cleared.\" << std::endl;\n",
        "\n",
        "    // Copy data back from device to host\n",
        "    matrix.copyDeviceToHost();\n",
        "    std::cout << \"Data copied from device to host.\" << std::endl;\n",
        "\n",
        "    // Verify the data\n",
        "    bool success = true;\n",
        "    for (size_t i = 0; i < 100; ++i) {\n",
        "        if (matrix[i] != static_cast<float>(i)) {\n",
        "            success = false;\n",
        "            break;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    if (success) {\n",
        "        std::cout << \"Test passed: Data verification successful.\" << std::endl;\n",
        "    } else {\n",
        "        std::cout << \"Test failed: Data verification unsuccessful.\" << std::endl;\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHIavDx8MGBc",
        "outputId": "b10c91a1-fe59-46d8-c8fd-2b6b230cb6df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu -o matrix.out"
      ],
      "metadata": {
        "id": "yPu37UJpMJr1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./matrix.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AyR2BDJM1mG",
        "outputId": "138c6b0d-4ea5-4f3c-dd01-cd0c3f7e25e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory allocated on host and device.\n",
            "Host data initialized.\n",
            "Data copied from host to device.\n",
            "Host data cleared.\n",
            "Data copied from device to host.\n",
            "Test passed: Data verification successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loss Function - Binary Cross Entropy in CUDA"
      ],
      "metadata": {
        "id": "iySk8qC3NI4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bce_cost.hh\n",
        "#pragma once\n",
        "#include \"matrix.hh\"\n",
        "\n",
        "class BCECost {\n",
        "public:\n",
        " float cost(Matrix predictions, Matrix target);\n",
        " Matrix dCost(Matrix predictions, Matrix target, Matrix dY);\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_Se3jZHNMx_",
        "outputId": "5a4a37da-cd9f-4f35-bbf6-430bced10499"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bce_cost.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two Kernels for calculating BCE in CUDA"
      ],
      "metadata": {
        "id": "UezKt8ucOG8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bce_cost.cu\n",
        "#include \"bce_cost.hh\"\n",
        "#include <cmath>\n",
        "#include <cassert>\n",
        "#include \"nn_exception.hh\"\n",
        "\n",
        "__global__ void binaryCrossEntropyCost(float* predictions, float* target, int size, float* cost) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < size) {\n",
        "        float pred = predictions[index];\n",
        "        pred = fmaxf(fminf(pred, 1.0f - 1e-7), 1e-7);\n",
        "\n",
        "        float partial_cost = target[index] * logf(pred)\n",
        "                + (1.0f - target[index]) * logf(1.0f - pred);\n",
        "        atomicAdd(cost, - partial_cost / size);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void dBinaryCrossEntropyCost(float* predictions, float* target, float* dY,\n",
        "                                        int size) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < size) {\n",
        "        // Clamp predictions to avoid division by zero\n",
        "        float pred = predictions[index];\n",
        "        pred = fmaxf(fminf(pred, 1.0f - 1e-7), 1e-7);\n",
        "\n",
        "        dY[index] = -1.0 * (target[index] / pred - (1 - target[index]) / (1 - pred));\n",
        "    }\n",
        "}\n",
        "\n",
        "//cost (or loss)\n",
        "float BCECost::cost(Matrix predictions, Matrix target) {\n",
        " assert(predictions.shape.x == target.shape.x);\n",
        "\n",
        " float* cost;\n",
        " cudaMallocManaged(&cost, sizeof(float));\n",
        " *cost = 0.0f;\n",
        "\n",
        " dim3 block_size(256);\n",
        " dim3 num_of_blocks((predictions.shape.x + block_size.x - 1) / block_size.x);\n",
        " binaryCrossEntropyCost<<<num_of_blocks, block_size>>>(predictions.data_device.get(),\n",
        "                target.data_device.get(),\n",
        "                predictions.shape.x, cost);\n",
        " cudaDeviceSynchronize();\n",
        " NNException::throwIfDeviceErrorsOccurred(\"Cannot compute binary cross entropy cost.\");\n",
        "\n",
        " float cost_value = *cost;\n",
        " cudaFree(cost);\n",
        "\n",
        " return cost_value;\n",
        "}\n",
        "\n",
        "//derivative of cost (aka derivative of loss)\n",
        "Matrix BCECost::dCost(Matrix predictions, Matrix target, Matrix dY) {\n",
        " assert(predictions.shape.x == target.shape.x);\n",
        "\n",
        " dim3 block_size(256);\n",
        " dim3 num_of_blocks((predictions.shape.x + block_size.x - 1) / block_size.x);\n",
        " dBinaryCrossEntropyCost<<<num_of_blocks, block_size>>>(predictions.data_device.get(),\n",
        "                 target.data_device.get(),\n",
        "                 dY.data_device.get(),\n",
        "                 predictions.shape.x);\n",
        " NNException::throwIfDeviceErrorsOccurred(\"Cannot compute derivative for binary cross entropy.\");\n",
        "\n",
        " return dY;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBH5dVtJOK_m",
        "outputId": "8e0df149-075c-4436-9001-b19f22f3a5f4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bce_cost.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include \"matrix.hh\"\n",
        "#include \"bce_cost.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "\n",
        "// Helper function to initialize a Matrix with data\n",
        "void initializeMatrix(Matrix& matrix, const std::vector<float>& data) {\n",
        "    for (size_t i = 0; i < data.size(); ++i) {\n",
        "        matrix[i] = data[i];\n",
        "    }\n",
        "    matrix.copyHostToDevice();\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the size of the data\n",
        "    const int size = 10;\n",
        "\n",
        "    // Create predictions and target data\n",
        "    std::vector<float> predictions_data = {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95};\n",
        "    std::vector<float> target_data = {0, 0, 1, 0, 1, 0, 1, 1, 1, 0};\n",
        "\n",
        "    // Create Matrix objects for predictions and targets\n",
        "    Matrix predictions(size, 1);\n",
        "    Matrix target(size, 1);\n",
        "    predictions.allocateMemory();\n",
        "    target.allocateMemory();\n",
        "\n",
        "    // Initialize matrices with data\n",
        "    initializeMatrix(predictions, predictions_data);\n",
        "    initializeMatrix(target, target_data);\n",
        "\n",
        "    // Compute the binary cross-entropy cost\n",
        "    BCECost bce_cost;\n",
        "    float cost_value = bce_cost.cost(predictions, target);\n",
        "    std::cout << \"Binary Cross-Entropy Cost: \" << cost_value << std::endl;\n",
        "\n",
        "    // Compute the gradient of the binary cross-entropy cost\n",
        "    Matrix dY(size, 1);\n",
        "    dY.allocateMemory();\n",
        "    Matrix dCost_matrix = bce_cost.dCost(predictions, target, dY);\n",
        "    dCost_matrix.copyDeviceToHost();\n",
        "\n",
        "    // Print the gradient values\n",
        "    std::cout << \"Gradient of Binary Cross-Entropy Cost: \";\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        std::cout << dCost_matrix[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asKfR8AMPOb4",
        "outputId": "d8c3203b-de5e-4e28-e631-3de90dabf5ce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu bce_cost.cu -o bce.out"
      ],
      "metadata": {
        "id": "G3mYcL5CPY1J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bce.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73rY40alQJFE",
        "outputId": "e7cd8fae-3a69-4210-cea3-a3968dd151ec"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross-Entropy Cost: 0.733365\n",
            "Gradient of Binary Cross-Entropy Cost: 1.11111 1.25 -3.33333 1.66667 -2 2.5 -1.42857 -1.25 -1.11111 20 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining the Model"
      ],
      "metadata": {
        "id": "lrLH70j9zj4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abstract class that will be inherited by all other NN-Layer classes"
      ],
      "metadata": {
        "id": "iz_fu24a0oPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nn_layer.hh\n",
        "#pragma once\n",
        "\n",
        "#include <iostream>\n",
        "#include \"matrix.hh\"\n",
        "\n",
        "class NNLayer {\n",
        "  protected:\n",
        "    std::string name;\n",
        "\n",
        "  public:\n",
        "    virtual ~NNLayer() { }\n",
        "    virtual Matrix& forward(Matrix& A) = 0;\n",
        "    virtual Matrix& backprop(Matrix& dZ, float learning_rate) = 0;\n",
        "\n",
        "    std::string getName() { return this->name; };\n",
        "};\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFre7TLjzlcx",
        "outputId": "cb829714-3eca-465f-c31f-67d9d9955aeb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing nn_layer.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear Layer"
      ],
      "metadata": {
        "id": "C34ZEqIKOlvb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linear_layer.hh\n",
        "#pragma once\n",
        "#include \"nn_layer.hh\"\n",
        "\n",
        "class LinearLayer : public NNLayer {\n",
        "  private:\n",
        "    const float weights_init_threshold = 0.01;\n",
        "\n",
        "    Matrix W;\n",
        "    Matrix b;\n",
        "    Matrix Z;\n",
        "    Matrix A;\n",
        "    Matrix dA;\n",
        "\n",
        "    void initializeBiasWithZeros();\n",
        "    void initializeWeightsRandomly();\n",
        "\n",
        "    void computeAndStoreBackpropError(Matrix& dZ);\n",
        "    void computeAndStoreLayerOutput(Matrix& A);\n",
        "    void updateWeights(Matrix& dZ, float learning_rate);\n",
        "    void updateBias(Matrix& dZ, float learning_rate);\n",
        "\n",
        "  public:\n",
        "    LinearLayer(std::string name, Shape W_shape);\n",
        "    virtual ~LinearLayer() override;\n",
        "\n",
        "    Matrix& forward(Matrix& A) override;\n",
        "    Matrix& backprop(Matrix& dZ, float learning_rate=0.01) override;\n",
        "\n",
        "    int getXDim() const;\n",
        "    int getYDim() const;\n",
        "\n",
        "    Matrix& getWeightsMatrix();\n",
        "    Matrix& getBiasVector();\n",
        "};\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu4wWqOa0wK8",
        "outputId": "54029ea4-2df2-418e-e3bb-484cec308232"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing linear_layer.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linear_layer.cu\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <cassert>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include \"linear_layer.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "\n",
        "__global__ void linearLayerForward( float* W, float* A, float* Z, float* b,\n",
        "\t\t\t\t\t\t\t\t\tint W_x_dim, int W_y_dim,\n",
        "\t\t\t\t\t\t\t\t\tint A_x_dim, int A_y_dim) {\n",
        "\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\tint Z_x_dim = A_x_dim;\n",
        "\tint Z_y_dim = W_y_dim;\n",
        "\n",
        "\tfloat Z_value = 0;\n",
        "\n",
        "\tif (row < Z_y_dim && col < Z_x_dim) {\n",
        "\t\tfor (int i = 0; i < W_x_dim; i++) {\n",
        "\t\t\tZ_value += W[row * W_x_dim + i] * A[i * A_x_dim + col];\n",
        "\t\t}\n",
        "\t\tZ[row * Z_x_dim + col] = Z_value + b[row];\n",
        "\t}\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void linearLayerBackprop(float* W, float* dZ, float *dA,\n",
        "\t\t\t\t\t\t\t\t\tint W_x_dim, int W_y_dim,\n",
        "\t\t\t\t\t\t\t\t\tint dZ_x_dim, int dZ_y_dim) {\n",
        "\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\t// W is treated as transposed\n",
        "\tint dA_x_dim = dZ_x_dim;\n",
        "\tint dA_y_dim = W_x_dim;\n",
        "\n",
        "\tfloat dA_value = 0.0f;\n",
        "\n",
        "\tif (row < dA_y_dim && col < dA_x_dim) {\n",
        "\t\tfor (int i = 0; i < W_y_dim; i++) {\n",
        "\t\t\tdA_value += W[i * W_x_dim + row] * dZ[i * dZ_x_dim + col];\n",
        "\t\t}\n",
        "\t\tdA[row * dA_x_dim + col] = dA_value;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void linearLayerUpdateWeights(  float* dZ, float* A, float* W,\n",
        "\t\t\t\t\t\t\t\t\t\t   int dZ_x_dim, int dZ_y_dim,\n",
        "\t\t\t\t\t\t\t\t\t\t   int A_x_dim, int A_y_dim,\n",
        "\t\t\t\t\t\t\t\t\t\t   float learning_rate) {\n",
        "\n",
        "\tint col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\tint row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "\t// A is treated as transposed\n",
        "\tint W_x_dim = A_y_dim;\n",
        "\tint W_y_dim = dZ_y_dim;\n",
        "\n",
        "\tfloat dW_value = 0.0f;\n",
        "\n",
        "\tif (row < W_y_dim && col < W_x_dim) {\n",
        "\t\tfor (int i = 0; i < dZ_x_dim; i++) {\n",
        "\t\t\tdW_value += dZ[row * dZ_x_dim + i] * A[col * A_x_dim + i];\n",
        "\t\t}\n",
        "\t\tW[row * W_x_dim + col] = W[row * W_x_dim + col] - learning_rate * (dW_value / A_x_dim);\n",
        "\t}\n",
        "}\n",
        "\n",
        "__global__ void linearLayerUpdateBias(  float* dZ, float* b,\n",
        "\t\t\t\t\t\t\t\t\t\tint dZ_x_dim, int dZ_y_dim,\n",
        "\t\t\t\t\t\t\t\t\t\tint b_x_dim,\n",
        "\t\t\t\t\t\t\t\t\t\tfloat learning_rate) {\n",
        "\tint index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "\tif (index < dZ_x_dim * dZ_y_dim) {\n",
        "\t\tint dZ_x = index % dZ_x_dim;\n",
        "\t\tint dZ_y = index / dZ_x_dim;\n",
        "\t\tatomicAdd(&b[dZ_y], - learning_rate * (dZ[dZ_y * dZ_x_dim + dZ_x] / dZ_x_dim));\n",
        "\t}\n",
        "}\n",
        "\n",
        "LinearLayer::LinearLayer(std::string name, Shape W_shape) :\n",
        "\tW(W_shape), b(W_shape.y, 1)\n",
        "{\n",
        "\tthis->name = name;\n",
        "\tb.allocateMemory();\n",
        "\tW.allocateMemory();\n",
        "\tinitializeBiasWithZeros();\n",
        "\tinitializeWeightsRandomly();\n",
        "}\n",
        "\n",
        "LinearLayer::~LinearLayer()\n",
        "{ }\n",
        "\n",
        "void LinearLayer::initializeWeightsRandomly() {\n",
        "\tstd::default_random_engine generator;\n",
        "\tstd::normal_distribution normal_distribution(0.0, 1.0);\n",
        "\n",
        "\tfor (int x = 0; x < W.shape.x; x++) {\n",
        "\t\tfor (int y = 0; y < W.shape.y; y++) {\n",
        "\t\t\tW[y * W.shape.x + x] = normal_distribution(generator) * weights_init_threshold;\n",
        "\t\t}\n",
        "\t}\n",
        "\n",
        "\tW.copyHostToDevice();\n",
        "}\n",
        "\n",
        "void LinearLayer::initializeBiasWithZeros() {\n",
        "\tfor (int x = 0; x < b.shape.x; x++) {\n",
        "\t\tb[x] = 0;\n",
        "\t}\n",
        "\n",
        "\tb.copyHostToDevice();\n",
        "}\n",
        "\n",
        "Matrix& LinearLayer::forward(Matrix& A) {\n",
        "\tassert(W.shape.x == A.shape.y);\n",
        "\n",
        "\tthis->A = A;\n",
        "\tShape Z_shape(A.shape.x, W.shape.y);\n",
        "\tZ.allocateMemoryIfNotAllocated(Z_shape);\n",
        "\n",
        "\tcomputeAndStoreLayerOutput(A);\n",
        "\tNNException::throwIfDeviceErrorsOccurred(\"Cannot perform linear layer forward propagation.\");\n",
        "\n",
        "\treturn Z;\n",
        "}\n",
        "\n",
        "void LinearLayer::computeAndStoreLayerOutput(Matrix& A) {\n",
        "    dim3 block_size(8, 8);\n",
        "    dim3 num_of_blocks(\t(Z.shape.x + block_size.x - 1) / block_size.x,\n",
        "\t\t\t\t\t\t(Z.shape.y + block_size.y - 1) / block_size.y);\n",
        "    linearLayerForward<<<num_of_blocks, block_size>>>(W.data_device.get(),\n",
        "                                                      A.data_device.get(),\n",
        "                                                      Z.data_device.get(),\n",
        "                                                      b.data_device.get(),\n",
        "                                                      W.shape.x, W.shape.y,\n",
        "                                                      A.shape.x, A.shape.y);\n",
        "}\n",
        "\n",
        "Matrix& LinearLayer::backprop(Matrix& dZ, float learning_rate) {\n",
        "\tdA.allocateMemoryIfNotAllocated(A.shape);\n",
        "\n",
        "\tcomputeAndStoreBackpropError(dZ);\n",
        "\tNNException::throwIfDeviceErrorsOccurred(\"Cannot perform back propagation.\");\n",
        "\n",
        "\tupdateBias(dZ, learning_rate);\n",
        "\tNNException::throwIfDeviceErrorsOccurred(\"Cannot perform bias update.\");\n",
        "\n",
        "\tupdateWeights(dZ, learning_rate);\n",
        "\tNNException::throwIfDeviceErrorsOccurred(\"Cannot perform weights update.\");\n",
        "\n",
        "\treturn dA;\n",
        "}\n",
        "\n",
        "void LinearLayer::computeAndStoreBackpropError(Matrix& dZ) {\n",
        "    dim3 block_size(8, 8);\n",
        "    dim3 num_of_blocks(\t(A.shape.x + block_size.x - 1) / block_size.x,\n",
        "\t\t\t\t\t\t(A.shape.y + block_size.y - 1) / block_size.y);\n",
        "    linearLayerBackprop<<<num_of_blocks, block_size>>>(W.data_device.get(),\n",
        "                                                       dZ.data_device.get(),\n",
        "                                                       dA.data_device.get(),\n",
        "                                                       W.shape.x, W.shape.y,\n",
        "                                                       dZ.shape.x, dZ.shape.y);\n",
        "}\n",
        "\n",
        "void LinearLayer::updateWeights(Matrix& dZ, float learning_rate) {\n",
        "    dim3 block_size(8, 8);\n",
        "    dim3 num_of_blocks(\t(W.shape.x + block_size.x - 1) / block_size.x,\n",
        "\t\t\t\t\t\t(W.shape.y + block_size.y - 1) / block_size.y);\n",
        "    linearLayerUpdateWeights<<<num_of_blocks, block_size>>>(dZ.data_device.get(),\n",
        "                                                            A.data_device.get(),\n",
        "                                                            W.data_device.get(),\n",
        "                                                            dZ.shape.x, dZ.shape.y,\n",
        "                                                            A.shape.x, A.shape.y,\n",
        "                                                            learning_rate);\n",
        "}\n",
        "\n",
        "void LinearLayer::updateBias(Matrix& dZ, float learning_rate) {\n",
        "    dim3 block_size(256);\n",
        "    dim3 num_of_blocks( (dZ.shape.y * dZ.shape.x + block_size.x - 1) / block_size.x);\n",
        "    linearLayerUpdateBias<<<num_of_blocks, block_size>>>(dZ.data_device.get(),\n",
        "                                                         b.data_device.get(),\n",
        "                                                         dZ.shape.x, dZ.shape.y,\n",
        "                                                         b.shape.x, learning_rate);\n",
        "}\n",
        "\n",
        "int LinearLayer::getXDim() const {\n",
        "\treturn W.shape.x;\n",
        "}\n",
        "\n",
        "int LinearLayer::getYDim() const {\n",
        "\treturn W.shape.y;\n",
        "}\n",
        "\n",
        "Matrix& LinearLayer::getWeightsMatrix() {\n",
        "    return W;\n",
        "}\n",
        "\n",
        "Matrix& LinearLayer::getBiasVector() {\n",
        "    return b;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HteZb61C3BGv",
        "outputId": "60ada6fc-8ec6-4278-c15a-3f0e4b88de4c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing linear_layer.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####main"
      ],
      "metadata": {
        "id": "wyhN8-lEPFiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"linear_layer.hh\"\n",
        "#include \"bce_cost.hh\"\n",
        "#include \"matrix.hh\"\n",
        "\n",
        "void printMatrix(Matrix& matrix, const std::string& name) {\n",
        "    matrix.copyDeviceToHost();\n",
        "    std::cout << name << \":\" << std::endl;\n",
        "    for (int i = 0; i < matrix.shape.x * matrix.shape.y; ++i) {\n",
        "        std::cout << matrix[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define input dimensions and initialize the layer\n",
        "    Shape input_shape(1, 3); // (1 rows, 3 columns, transposed vector)\n",
        "    Shape weight_shape(3, 1); // shape of weights, resulting in a 1x1 output\n",
        "\n",
        "    LinearLayer layer(\"test_layer\", weight_shape);\n",
        "\n",
        "    // Allocate memory for input and output\n",
        "    Matrix input(input_shape);\n",
        "    input.allocateMemory();\n",
        "    input[0] = 0.1f; input[1] = 0.2f; input[2] = 0.3f;\n",
        "    input.copyHostToDevice();\n",
        "\n",
        "    // Allocate memory for target\n",
        "    Matrix target(Shape(1, 1)); // 1x1 target matrix\n",
        "    target.allocateMemory();\n",
        "    target[0] = 0.0f;\n",
        "    target.copyHostToDevice();\n",
        "\n",
        "    // Print initial weights and biases\n",
        "    printMatrix(layer.getWeightsMatrix(), \"Initial Weights\");\n",
        "    printMatrix(layer.getBiasVector(), \"Initial Biases\");\n",
        "\n",
        "    // Perform forward pass\n",
        "    Matrix& output = layer.forward(input);\n",
        "    output.copyDeviceToHost();\n",
        "\n",
        "    // Print forward pass output\n",
        "    std::cout << \"Forward pass output:\" << std::endl;\n",
        "    for (int i = 0; i < output.shape.x * output.shape.y; ++i) {\n",
        "        std::cout << output[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Calculate BCE loss\n",
        "    BCECost bce;\n",
        "    float loss = bce.cost(output, target);\n",
        "    std::cout << \"Binary Cross Entropy Loss: \" << loss << std::endl;\n",
        "\n",
        "    // Calculate gradient of BCE loss\n",
        "    Matrix dZ(output.shape);\n",
        "    dZ.allocateMemory();\n",
        "    bce.dCost(output, target, dZ);\n",
        "\n",
        "    // Perform backpropagation\n",
        "    float learning_rate = 0.01f;\n",
        "    Matrix& dA = layer.backprop(dZ, learning_rate);\n",
        "    dA.copyDeviceToHost();\n",
        "\n",
        "    // Print backpropagation output (dA)\n",
        "    std::cout << \"Backpropagation output (dA):\" << std::endl;\n",
        "    for (int i = 0; i < dA.shape.x * dA.shape.y; ++i) {\n",
        "        std::cout << dA[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "\n",
        "    // Print updated weights and biases\n",
        "    printMatrix(layer.getWeightsMatrix(), \"Updated Weights\");\n",
        "    printMatrix(layer.getBiasVector(), \"Updated Biases\");\n",
        "\n",
        "    return 0;\n",
        "}\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QsFDz3h3TG_",
        "outputId": "f2cbcdb6-9722-4322-902e-3a8413001e9e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu bce_cost.cu linear_layer.cu -o ll.out\n",
        "!./ll.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bmG0lGW3ZHT",
        "outputId": "b17734b6-3be3-47ec-8a8b-cbe64af5ca6d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "-0.00121966 -0.0108682 0.0068429 \n",
            "Initial Biases:\n",
            "0 \n",
            "Forward pass output:\n",
            "-0.000242732 \n",
            "Binary Cross Entropy Loss: 1.19209e-07\n",
            "Backpropagation output (dA):\n",
            "-0.00121966 -0.0108682 0.0068429 \n",
            "Updated Weights:\n",
            "-0.00221966 -0.0128682 0.0038429 \n",
            "Updated Biases:\n",
            "-0.01 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train a Naive Linear Model + Quick Check\n",
        "\n",
        "because BCE is clamped, this will attempt to predict a number below zero if the target is zero, and a value above one if the target is 1"
      ],
      "metadata": {
        "id": "6uKCdA6kPKDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"linear_layer.hh\"\n",
        "#include \"bce_cost.hh\"\n",
        "#include \"matrix.hh\"\n",
        "\n",
        "void printMatrix(Matrix& matrix, const std::string& name) {\n",
        "    matrix.copyDeviceToHost();\n",
        "    std::cout << name << \":\" << std::endl;\n",
        "    for (int i = 0; i < matrix.shape.x * matrix.shape.y; ++i) {\n",
        "        std::cout << matrix[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define input dimensions and initialize the layer\n",
        "    Shape input_shape(1, 3); // (1 rows, 3 columns, transposed vector)\n",
        "    Shape weight_shape(3, 1); // shape of weights, resulting in a 1x1 output\n",
        "\n",
        "    LinearLayer layer(\"test_layer\", weight_shape);\n",
        "\n",
        "    // Allocate memory for input and output\n",
        "    Matrix input(input_shape);\n",
        "    input.allocateMemory();\n",
        "    input[0] = 0.1f; input[1] = 0.2f; input[2] = 0.3f;\n",
        "    input.copyHostToDevice();\n",
        "\n",
        "    // Allocate memory for target\n",
        "    Matrix target(Shape(1, 1)); // 1x1 target matrix\n",
        "    target.allocateMemory();\n",
        "    target[0] = 0.0f;\n",
        "    target.copyHostToDevice();\n",
        "\n",
        "    // Print initial weights and biases\n",
        "    printMatrix(layer.getWeightsMatrix(), \"Initial Weights\");\n",
        "    printMatrix(layer.getBiasVector(), \"Initial Biases\");\n",
        "\n",
        "    // Training loop\n",
        "    for (int i = 0; i < 3; ++i) {\n",
        "        // Perform forward pass\n",
        "        Matrix& output = layer.forward(input);\n",
        "        output.copyDeviceToHost();\n",
        "\n",
        "        // Print forward pass output\n",
        "        std::cout << \"Forward pass output:\" << std::endl;\n",
        "        for (int j = 0; j < output.shape.x * output.shape.y; ++j) {\n",
        "            std::cout << output[j] << \" \";\n",
        "        }\n",
        "        std::cout << std::endl;\n",
        "\n",
        "        // Calculate BCE loss\n",
        "        BCECost bce;\n",
        "        float loss = bce.cost(output, target);\n",
        "        std::cout << \"Loss at iteration \" << i << \": \" << loss << std::endl;\n",
        "\n",
        "        // Calculate gradient of BCE loss\n",
        "        Matrix dZ(output.shape);\n",
        "        dZ.allocateMemory();\n",
        "        bce.dCost(output, target, dZ);\n",
        "\n",
        "        // Perform backpropagation\n",
        "        float learning_rate = 0.000001f;\n",
        "        layer.backprop(dZ, learning_rate);\n",
        "    }\n",
        "\n",
        "    // Print updated weights and biases\n",
        "    printMatrix(layer.getWeightsMatrix(), \"Updated Weights\");\n",
        "    printMatrix(layer.getBiasVector(), \"Updated Biases\");\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1iSMPExPN5P",
        "outputId": "4102c576-b8a2-46d2-f565-e74d88f4f7c0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu bce_cost.cu linear_layer.cu -o ll.out\n",
        "!./ll.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCsKwOFAPbu_",
        "outputId": "07e55fe6-b664-4209-83cc-ee108e0bafaa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Weights:\n",
            "-0.00121966 -0.0108682 0.0068429 \n",
            "Initial Biases:\n",
            "0 \n",
            "Forward pass output:\n",
            "-0.000242732 \n",
            "Loss at iteration 0: 1.19209e-07\n",
            "Forward pass output:\n",
            "-0.000243872 \n",
            "Loss at iteration 1: 1.19209e-07\n",
            "Forward pass output:\n",
            "-0.000245012 \n",
            "Loss at iteration 2: 1.19209e-07\n",
            "Updated Weights:\n",
            "-0.00121996 -0.0108688 0.006842 \n",
            "Updated Biases:\n",
            "-3e-06 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sigmoid Function"
      ],
      "metadata": {
        "id": "UYQ0OAN9PlEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sigmoid_activation.hh\n",
        "#pragma once\n",
        "\n",
        "#include \"nn_layer.hh\"\n",
        "\n",
        "class SigmoidActivation : public NNLayer {\n",
        "private:\n",
        "\tMatrix A;\n",
        "\n",
        "\tMatrix Z;\n",
        "\tMatrix dZ;\n",
        "\n",
        "public:\n",
        "\tSigmoidActivation(std::string name);\n",
        "\t~SigmoidActivation();\n",
        "\n",
        "\tMatrix& forward(Matrix& Z);\n",
        "\tMatrix& backprop(Matrix& dA, float learning_rate = 0.01);\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLlYFA52PeG1",
        "outputId": "157e7945-cb5a-41cf-b856-3acb7c5ba45f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing sigmoid_activation.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile sigmoid_activation.cu\n",
        "#include \"sigmoid_activation.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "#include \"linear_layer.hh\"\n",
        "\n",
        "#include <iostream>\n",
        "#include <random>\n",
        "#include <cassert>\n",
        "#include <cmath>\n",
        "#include <algorithm>\n",
        "#include \"sigmoid_activation.hh\"\n",
        "\n",
        "__device__ float sigmoid(float x) {\n",
        "    return 1.0f / (1 + exp(-x));\n",
        "}\n",
        "\n",
        "__global__ void sigmoidActivationForward(float* Z, float* A,\n",
        "                                         int Z_x_dim, int Z_y_dim) {\n",
        "\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < Z_x_dim * Z_y_dim) {\n",
        "        A[index] = sigmoid(Z[index]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void sigmoidActivationBackprop(float* Z, float* dA, float* dZ,\n",
        "                                          int Z_x_dim, int Z_y_dim) {\n",
        "\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < Z_x_dim * Z_y_dim) {\n",
        "        dZ[index] = dA[index] * sigmoid(Z[index]) * (1 - sigmoid(Z[index]));\n",
        "    }\n",
        "}\n",
        "\n",
        "SigmoidActivation::SigmoidActivation(std::string name) {\n",
        "    this->name = name;\n",
        "}\n",
        "\n",
        "SigmoidActivation::~SigmoidActivation()\n",
        "{ }\n",
        "\n",
        "Matrix& SigmoidActivation::forward(Matrix& Z) {\n",
        "    this->Z = Z;\n",
        "    A.allocateMemoryIfNotAllocated(Z.shape);\n",
        "\n",
        "    dim3 block_size(256);\n",
        "    dim3 num_of_blocks((Z.shape.y * Z.shape.x + block_size.x - 1) / block_size.x);\n",
        "\n",
        "    sigmoidActivationForward<<<num_of_blocks, block_size>>>(Z.data_device.get(), A.data_device.get(),\n",
        "                                                            Z.shape.x, Z.shape.y);\n",
        "    NNException::throwIfDeviceErrorsOccurred(\"Cannot perform sigmoid forward propagation.\");\n",
        "\n",
        "    return A;\n",
        "}\n",
        "\n",
        "Matrix& SigmoidActivation::backprop(Matrix& dA, float learning_rate) {\n",
        "    dZ.allocateMemoryIfNotAllocated(Z.shape);\n",
        "\n",
        "    dim3 block_size(256);\n",
        "    dim3 num_of_blocks((Z.shape.y * Z.shape.x + block_size.x - 1) / block_size.x);\n",
        "    sigmoidActivationBackprop<<<num_of_blocks, block_size>>>(Z.data_device.get(), dA.data_device.get(),\n",
        "                                                             dZ.data_device.get(),\n",
        "                                                             Z.shape.x, Z.shape.y);\n",
        "    NNException::throwIfDeviceErrorsOccurred(\"Cannot perform sigmoid back propagation\");\n",
        "\n",
        "    return dZ;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSH5BPxGP7xC",
        "outputId": "9282acfd-6e02-420d-9101-fbb6785a49d3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sigmoid_activation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"sigmoid_activation.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "#include \"matrix.hh\"\n",
        "#include \"linear_layer.hh\"\n",
        "\n",
        "void printMatrix(Matrix& matrix, const std::string& name) {\n",
        "    matrix.copyDeviceToHost();\n",
        "    std::cout << name << \":\" << std::endl;\n",
        "    for (int i = 0; i < matrix.shape.x * matrix.shape.y; ++i) {\n",
        "        std::cout << matrix[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define input dimensions and initialize the matrix\n",
        "    Shape input_shape(1, 3); // (1 rows, 3 columns)\n",
        "\n",
        "    // Initialize SigmoidActivation\n",
        "    SigmoidActivation sigmoid(\"sigmoid_activation\");\n",
        "\n",
        "    // Allocate memory for input matrix\n",
        "    Matrix input(input_shape);\n",
        "    input.allocateMemory();\n",
        "    input[0] = -1.0f; input[1] = 0.0f; input[2] = 1.0f;\n",
        "    input.copyHostToDevice();\n",
        "\n",
        "    // Perform forward pass\n",
        "    Matrix& output = sigmoid.forward(input);\n",
        "    output.copyDeviceToHost();\n",
        "\n",
        "    // Print forward pass output\n",
        "    printMatrix(output, \"Forward pass output\");\n",
        "\n",
        "    // Allocate memory for gradient matrix\n",
        "    Matrix dA(output.shape);\n",
        "    dA.allocateMemory();\n",
        "    dA[0] = 0.1f; dA[1] = 0.2f; dA[2] = 0.3f;\n",
        "    dA.copyHostToDevice();\n",
        "\n",
        "    // Perform backward pass\n",
        "    Matrix& dZ = sigmoid.backprop(dA, 0.01f);\n",
        "    dZ.copyDeviceToHost();\n",
        "\n",
        "    // Print backward pass output\n",
        "    printMatrix(dZ, \"Backward pass output\");\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC5okojVQVZR",
        "outputId": "d3e144b7-056c-4290-891f-b9942c4fea42"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu bce_cost.cu sigmoid_activation.cu -o sig.out\n",
        "!./sig.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flOc2kC6QZ2Q",
        "outputId": "6ecdd217-59f5-47c6-eaba-8f2ebad1b6e5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass output:\n",
            "0.268941 0.5 0.731059 \n",
            "Backward pass output:\n",
            "0.0196612 0.05 0.0589836 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###ReLU Activation"
      ],
      "metadata": {
        "id": "7XLzeD8gRO5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile relu_activation.hh\n",
        "#pragma once\n",
        "\n",
        "#include \"nn_layer.hh\"\n",
        "\n",
        "class ReLUActivation : public NNLayer {\n",
        "private:\n",
        "\tMatrix A;\n",
        "\n",
        "\tMatrix Z;\n",
        "\tMatrix dZ;\n",
        "\n",
        "public:\n",
        "\tReLUActivation(std::string name);\n",
        "\t~ReLUActivation();\n",
        "\n",
        "\tMatrix& forward(Matrix& Z);\n",
        "\tMatrix& backprop(Matrix& dA, float learning_rate = 0.01);\n",
        "};"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3mAU-CyRQxn",
        "outputId": "7edbc6f1-b4be-46d5-a59c-e64719984b13"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing relu_activation.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile relu_activation.cu\n",
        "#include \"relu_activation.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "\n",
        "__global__ void reluActivationForward(float* Z, float* A,\n",
        "                                      int Z_x_dim, int Z_y_dim) {\n",
        "\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < Z_x_dim * Z_y_dim) {\n",
        "        A[index] = fmaxf(Z[index], 0);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void reluActivationBackprop(float* Z, float* dA, float* dZ,\n",
        "                                       int Z_x_dim, int Z_y_dim) {\n",
        "\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (index < Z_x_dim * Z_y_dim) {\n",
        "        if (Z[index] > 0) {\n",
        "            dZ[index] = dA[index];\n",
        "        }\n",
        "        else {\n",
        "            dZ[index] = 0;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "ReLUActivation::ReLUActivation(std::string name) {\n",
        "    this->name = name;\n",
        "}\n",
        "\n",
        "ReLUActivation::~ReLUActivation() { }\n",
        "\n",
        "Matrix& ReLUActivation::forward(Matrix& Z) {\n",
        "    this->Z = Z;\n",
        "    A.allocateMemoryIfNotAllocated(Z.shape);\n",
        "\n",
        "    dim3 block_size(256);\n",
        "    dim3 num_of_blocks((Z.shape.y * Z.shape.x + block_size.x - 1) / block_size.x);\n",
        "\n",
        "    reluActivationForward<<<num_of_blocks, block_size>>>(Z.data_device.get(), A.data_device.get(),\n",
        "                                                         Z.shape.x, Z.shape.y);\n",
        "    NNException::throwIfDeviceErrorsOccurred(\"Cannot perform ReLU forward propagation.\");\n",
        "\n",
        "    return A;\n",
        "}\n",
        "\n",
        "Matrix& ReLUActivation::backprop(Matrix& dA, float learning_rate) {\n",
        "    dZ.allocateMemoryIfNotAllocated(Z.shape);\n",
        "\n",
        "    dim3 block_size(256);\n",
        "    dim3 num_of_blocks((Z.shape.y * Z.shape.x + block_size.x - 1) / block_size.x);\n",
        "    reluActivationBackprop<<<num_of_blocks, block_size>>>(Z.data_device.get(), dA.data_device.get(),\n",
        "                                                          dZ.data_device.get(),\n",
        "                                                          Z.shape.x, Z.shape.y);\n",
        "    NNException::throwIfDeviceErrorsOccurred(\"Cannot perform ReLU back propagation\");\n",
        "\n",
        "    return dZ;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01281kOlRVph",
        "outputId": "2a9c82e1-3c25-47f0-dbee-80fa7daef7af"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting relu_activation.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"relu_activation.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "#include \"matrix.hh\"\n",
        "#include \"sigmoid_activation.hh\"\n",
        "#include \"linear_layer.hh\"\n",
        "\n",
        "void printMatrix(Matrix& matrix, const std::string& name) {\n",
        "    matrix.copyDeviceToHost();\n",
        "    std::cout << name << \":\" << std::endl;\n",
        "    for (int i = 0; i < matrix.shape.x * matrix.shape.y; ++i) {\n",
        "        std::cout << matrix[i] << \" \";\n",
        "    }\n",
        "    std::cout << std::endl;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define input dimensions and initialize the matrix\n",
        "    Shape input_shape(1, 3); // (1 rows, 3 columns)\n",
        "\n",
        "    // Initialize ReLUActivation\n",
        "    ReLUActivation relu(\"relu_activation\");\n",
        "\n",
        "    // Allocate memory for input matrix\n",
        "    Matrix input(input_shape);\n",
        "    input.allocateMemory();\n",
        "    input[0] = -1.0f; input[1] = 0.0f; input[2] = 1.0f;\n",
        "    input.copyHostToDevice();\n",
        "\n",
        "    // Perform forward pass\n",
        "    Matrix& output = relu.forward(input);\n",
        "    output.copyDeviceToHost();\n",
        "\n",
        "    // Print forward pass output\n",
        "    printMatrix(output, \"Forward pass output\");\n",
        "\n",
        "    // Allocate memory for gradient matrix\n",
        "    Matrix dA(output.shape);\n",
        "    dA.allocateMemory();\n",
        "    dA[0] = 0.1f; dA[1] = 0.2f; dA[2] = 0.3f;\n",
        "    dA.copyHostToDevice();\n",
        "\n",
        "    // Perform backward pass\n",
        "    Matrix& dZ = relu.backprop(dA, 0.01f);\n",
        "    dZ.copyDeviceToHost();\n",
        "\n",
        "    // Print backward pass output\n",
        "    printMatrix(dZ, \"Backward pass output\");\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFkxG78uRhsY",
        "outputId": "ae45ca1a-93f3-4dac-adf4-6a54c389e381"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu bce_cost.cu relu_activation.cu -o sig.out\n",
        "!./sig.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwIZVh0JRip6",
        "outputId": "bdc72c1b-ce8c-457a-efe1-20655a797d7e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward pass output:\n",
            "0 0 1 \n",
            "Backward pass output:\n",
            "0 0 0.3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Putting everything together\n",
        "\n",
        "Now we can put this all together to train a model. To do that we'll define two more (very small) abstractions–one for the model and one for the dataset"
      ],
      "metadata": {
        "id": "IicEfL4xSMFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural Network"
      ],
      "metadata": {
        "id": "KOdaWm50ShSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile neural_network.hh\n",
        "#pragma once\n",
        "\n",
        "#include \"nn_layer.hh\"\n",
        "#include <vector>\n",
        "#include \"bce_cost.hh\"\n",
        "#include \"matrix.hh\" // Ensure Matrix is defined or included if it's a separate file.\n",
        "\n",
        "class NeuralNetwork {\n",
        "private:\n",
        "    std::vector<NNLayer*> layers; // Specify the type of elements in the vector\n",
        "    BCECost bce_cost;\n",
        "\n",
        "    Matrix Y;\n",
        "    Matrix dY;\n",
        "    float learning_rate;\n",
        "\n",
        "public:\n",
        "    NeuralNetwork(float learning_rate = 0.01);\n",
        "    ~NeuralNetwork();\n",
        "\n",
        "    Matrix forward(Matrix X);\n",
        "    void backprop(Matrix predictions, Matrix target);\n",
        "\n",
        "    void addLayer(NNLayer *layer);\n",
        "    std::vector<NNLayer*> getLayers() const; // Return a vector of pointers to NNLayer\n",
        "};\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSgeCeLxSNoH",
        "outputId": "65c856a6-4135-4d28-c387-6f690510aea3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting neural_network.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile neural_network.cu\n",
        "#include \"neural_network.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "#include <vector>\n",
        "\n",
        "NeuralNetwork::NeuralNetwork(float learning_rate) :\n",
        "    learning_rate(learning_rate)\n",
        "{ }\n",
        "\n",
        "NeuralNetwork::~NeuralNetwork() {\n",
        "    for (auto layer : layers) {\n",
        "        delete layer;\n",
        "    }\n",
        "}\n",
        "\n",
        "void NeuralNetwork::addLayer(NNLayer* layer) {\n",
        "    this->layers.push_back(layer);\n",
        "}\n",
        "\n",
        "Matrix NeuralNetwork::forward(Matrix X) {\n",
        "    Matrix Z = X;\n",
        "\n",
        "    for (auto layer : layers) {\n",
        "        Z = layer->forward(Z);\n",
        "    }\n",
        "\n",
        "    Y = Z;\n",
        "    return Y;\n",
        "}\n",
        "\n",
        "void NeuralNetwork::backprop(Matrix predictions, Matrix target) {\n",
        "    dY.allocateMemoryIfNotAllocated(predictions.shape);\n",
        "    Matrix error = bce_cost.dCost(predictions, target, dY);\n",
        "\n",
        "    for (auto it = this->layers.rbegin(); it != this->layers.rend(); ++it) {\n",
        "        error = (*it)->backprop(error, learning_rate);\n",
        "    }\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "std::vector<NNLayer*> NeuralNetwork::getLayers() const {\n",
        "    return layers;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUSh_CnwSlM3",
        "outputId": "6db98317-a07e-4a9c-e206-cfef6587c2ce"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting neural_network.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dataset"
      ],
      "metadata": {
        "id": "ZY6xVgL6Sm-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coordinates_dataset.hh\n",
        "#pragma once\n",
        "\n",
        "#include \"matrix.hh\"\n",
        "#include \"shape.hh\"\n",
        "#include <vector>\n",
        "#include <cstdlib>\n",
        "#include <ctime>\n",
        "\n",
        "class CoordinatesDataset {\n",
        "private:\n",
        "    size_t batch_size;\n",
        "    size_t number_of_batches;\n",
        "\n",
        "    std::vector<Matrix> batches; // Specify the type of elements in the vector\n",
        "    std::vector<Matrix> targets; // Specify the type of elements in the vector\n",
        "\n",
        "public:\n",
        "    CoordinatesDataset(size_t batch_size, size_t number_of_batches);\n",
        "\n",
        "    int getNumOfBatches();\n",
        "    std::vector<Matrix>& getBatches(); // Return reference to vector of Matrix\n",
        "    std::vector<Matrix>& getTargets(); // Return reference to vector of Matrix\n",
        "};\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6iWatCmSntn",
        "outputId": "af1da71f-33dd-4040-f486-c3f715fb6bf9"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coordinates_dataset.hh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile coordinates_dataset.cu\n",
        "#include \"coordinates_dataset.hh\"\n",
        "#include <vector>\n",
        "\n",
        "CoordinatesDataset::CoordinatesDataset(size_t batch_size, size_t number_of_batches) :\n",
        "    batch_size(batch_size), number_of_batches(number_of_batches)\n",
        "{\n",
        "    for (int i = 0; i < number_of_batches; i++) {\n",
        "        batches.push_back(Matrix(Shape(batch_size, 2)));\n",
        "        targets.push_back(Matrix(Shape(batch_size, 1)));\n",
        "\n",
        "        batches[i].allocateMemory();\n",
        "        targets[i].allocateMemory();\n",
        "\n",
        "        for (int k = 0; k < batch_size; k++) {\n",
        "            batches[i][k] = static_cast<float>(rand()) / RAND_MAX - 0.5;\n",
        "            batches[i][batches[i].shape.x + k] = static_cast<float>(rand()) / RAND_MAX - 0.5;\n",
        "\n",
        "            if ((batches[i][k] > 0 && batches[i][batches[i].shape.x + k] > 0) ||\n",
        "                (batches[i][k] < 0 && batches[i][batches[i].shape.x + k] < 0)) {\n",
        "                targets[i][k] = 1;\n",
        "            }\n",
        "            else {\n",
        "                targets[i][k] = 0;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        batches[i].copyHostToDevice();\n",
        "        targets[i].copyHostToDevice();\n",
        "    }\n",
        "}\n",
        "\n",
        "int CoordinatesDataset::getNumOfBatches() {\n",
        "    return number_of_batches;\n",
        "}\n",
        "\n",
        "std::vector<Matrix>& CoordinatesDataset::getBatches() {\n",
        "    return batches;\n",
        "}\n",
        "\n",
        "std::vector<Matrix>& CoordinatesDataset::getTargets() {\n",
        "    return targets;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drC5oEHcS4Bi",
        "outputId": "74c8af5f-790b-4ef0-f92d-7961f5131943"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting coordinates_dataset.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training NN on the dataset, using the model, and evaluating it"
      ],
      "metadata": {
        "id": "kdOdiiczS6_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile main.cu\n",
        "#include \"matrix.hh\"\n",
        "#include \"shape.hh\"\n",
        "#include \"neural_network.hh\"\n",
        "#include \"linear_layer.hh\"\n",
        "#include \"relu_activation.hh\"\n",
        "#include \"sigmoid_activation.hh\"\n",
        "#include \"nn_exception.hh\"\n",
        "#include \"bce_cost.hh\"\n",
        "#include \"coordinates_dataset.hh\"\n",
        "#include <vector>\n",
        "#include <ctime> // Include for time()\n",
        "#include <iostream> // Include for std::cout\n",
        "\n",
        "float computeAccuracy(const Matrix& predictions, const Matrix& targets);\n",
        "\n",
        "int main() {\n",
        "    srand(static_cast<unsigned int>(time(NULL))); // Corrected for proper use of time()\n",
        "\n",
        "    CoordinatesDataset dataset(100, 21);\n",
        "    BCECost bce_cost;\n",
        "\n",
        "    NeuralNetwork nn;\n",
        "    nn.addLayer(new LinearLayer(\"linear_1\", Shape(2, 30)));\n",
        "    nn.addLayer(new ReLUActivation(\"relu_1\"));\n",
        "    nn.addLayer(new LinearLayer(\"linear_2\", Shape(30, 1)));\n",
        "    nn.addLayer(new SigmoidActivation(\"sigmoid_output\"));\n",
        "\n",
        "    // Network training\n",
        "    Matrix Y;\n",
        "    for (int epoch = 0; epoch < 1001; epoch++) {\n",
        "        float cost = 0.0f;\n",
        "\n",
        "        for (int batch = 0; batch < dataset.getNumOfBatches() - 1; batch++) {\n",
        "            Y = nn.forward(dataset.getBatches().at(batch));\n",
        "            nn.backprop(Y, dataset.getTargets().at(batch));\n",
        "            cost += bce_cost.cost(Y, dataset.getTargets().at(batch));\n",
        "        }\n",
        "\n",
        "        if (epoch % 100 == 0) {\n",
        "            std::cout << \"Epoch: \" << epoch\n",
        "                      << \", Cost: \" << cost / dataset.getNumOfBatches()\n",
        "                      << std::endl;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Compute accuracy\n",
        "    Y = nn.forward(dataset.getBatches().at(dataset.getNumOfBatches() - 1));\n",
        "    Y.copyDeviceToHost();\n",
        "\n",
        "    float accuracy = computeAccuracy(\n",
        "        Y, dataset.getTargets().at(dataset.getNumOfBatches() - 1));\n",
        "    std::cout << \"Accuracy: \" << accuracy << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "float computeAccuracy(const Matrix& predictions, const Matrix& targets) {\n",
        "    int m = predictions.shape.x;\n",
        "    int correct_predictions = 0;\n",
        "\n",
        "    for (int i = 0; i < m; i++) {\n",
        "        float prediction = predictions[i] > 0.5f ? 1.0f : 0.0f;\n",
        "        if (prediction == targets[i]) {\n",
        "            correct_predictions++;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return static_cast<float>(correct_predictions) / m;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_jXAUFBTBzU",
        "outputId": "c5eb1757-2c28-4829-8e12-513e1efae279"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting main.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc main.cu matrix.cu shape.cu bce_cost.cu sigmoid_activation.cu relu_activation.cu linear_layer.cu coordinates_dataset.cu neural_network.cu -o main.out\n",
        "!./main.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjDMVKwTFsP",
        "outputId": "6b47fcaa-36b2-472e-b923-14d7a6e17aaf"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Cost: 0.660185\n",
            "Epoch: 100, Cost: 0.660049\n",
            "Epoch: 200, Cost: 0.659814\n",
            "Epoch: 300, Cost: 0.659071\n",
            "Epoch: 400, Cost: 0.65675\n",
            "Epoch: 500, Cost: 0.649827\n",
            "Epoch: 600, Cost: 0.631669\n",
            "Epoch: 700, Cost: 0.595266\n",
            "Epoch: 800, Cost: 0.540859\n",
            "Epoch: 900, Cost: 0.464867\n",
            "Epoch: 1000, Cost: 0.378224\n",
            "Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./main.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKDjA8ISb6Gv",
        "outputId": "e56c7702-e78d-488e-9b0c-0d72e944505c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==20394== NVPROF is profiling process 20394, command: ./main.out\n",
            "Epoch: 0, Cost: 0.660178\n",
            "Epoch: 100, Cost: 0.659858\n",
            "Epoch: 200, Cost: 0.659594\n",
            "Epoch: 300, Cost: 0.658729\n",
            "Epoch: 400, Cost: 0.656002\n",
            "Epoch: 500, Cost: 0.647854\n",
            "Epoch: 600, Cost: 0.627464\n",
            "Epoch: 700, Cost: 0.591304\n",
            "Epoch: 800, Cost: 0.544744\n",
            "Epoch: 900, Cost: 0.486751\n",
            "Epoch: 1000, Cost: 0.412162\n",
            "Accuracy: 0.88\n",
            "==20394== Profiling application: ./main.out\n",
            "==20394== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   83.90%  3.36726s     20020  168.20us  98.304us  8.2207ms  binaryCrossEntropyCost(float*, float*, int, float*)\n",
            "                    3.86%  154.86ms     40040  3.8670us  3.0390us  7.4890us  linearLayerUpdateWeights(float*, float*, float*, int, int, int, int, float)\n",
            "                    3.31%  132.88ms     40040  3.3180us  1.3430us  9.1840us  linearLayerUpdateBias(float*, float*, int, int, int, float)\n",
            "                    2.57%  103.04ms     40042  2.5730us  1.9200us  5.2800us  linearLayerForward(float*, float*, float*, float*, int, int, int, int)\n",
            "                    2.10%  84.439ms     40040  2.1080us  1.2790us  4.9610us  linearLayerBackprop(float*, float*, float*, int, int, int, int)\n",
            "                    1.07%  43.036ms     20020  2.1490us  1.6960us  4.1930us  dBinaryCrossEntropyCost(float*, float*, float*, int)\n",
            "                    0.86%  34.686ms     20020  1.7320us  1.3750us  3.2970us  sigmoidActivationBackprop(float*, float*, float*, int, int)\n",
            "                    0.83%  33.159ms     20020  1.6560us  1.3110us  3.1680us  reluActivationBackprop(float*, float*, float*, int, int)\n",
            "                    0.78%  31.154ms     20021  1.5560us  1.2160us  2.9770us  sigmoidActivationForward(float*, float*, int, int)\n",
            "                    0.72%  28.702ms     20021  1.4330us  1.1510us  3.0080us  reluActivationForward(float*, float*, int, int)\n",
            "                    0.00%  32.544us        46     707ns     671ns  1.1510us  [CUDA memcpy HtoD]\n",
            "                    0.00%  1.7280us         1  1.7280us  1.7280us  1.7280us  [CUDA memcpy DtoH]\n",
            "      API calls:   39.29%  3.75509s     40040  93.783us  3.3840us  8.2289ms  cudaDeviceSynchronize\n",
            "                   24.81%  2.37185s     20075  118.15us  3.9370us  7.3213ms  cudaFree\n",
            "                   22.98%  2.19601s    280284  7.8340us  3.0380us  10.690ms  cudaLaunchKernel\n",
            "                    9.73%  929.86ms     20020  46.446us  26.634us  20.303ms  cudaMallocManaged\n",
            "                    2.31%  220.40ms        55  4.0073ms  3.6780us  220.08ms  cudaMalloc\n",
            "                    0.88%  84.397ms    280386     301ns     139ns  1.0496ms  cudaGetLastError\n",
            "                    0.00%  390.32us        47  8.3040us  5.6480us  38.146us  cudaMemcpy\n",
            "                    0.00%  196.32us       114  1.7220us     208ns  75.384us  cuDeviceGetAttribute\n",
            "                    0.00%  15.382us         1  15.382us  15.382us  15.382us  cuDeviceGetName\n",
            "                    0.00%  7.5200us         1  7.5200us  7.5200us  7.5200us  cuDeviceGetPCIBusId\n",
            "                    0.00%  6.4980us         1  6.4980us  6.4980us  6.4980us  cuDeviceTotalMem\n",
            "                    0.00%  2.1270us         3     709ns     428ns  1.2090us  cuDeviceGetCount\n",
            "                    0.00%  1.2150us         1  1.2150us  1.2150us  1.2150us  cuModuleGetLoadingMode\n",
            "                    0.00%  1.0730us         2     536ns     331ns     742ns  cuDeviceGet\n",
            "                    0.00%     496ns         1     496ns     496ns     496ns  cuDeviceGetUuid\n",
            "\n",
            "==20394== Unified Memory profiling result:\n",
            "Device \"Tesla T4 (0)\"\n",
            "   Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
            "   40040  32.000KB  4.0000KB  60.000KB  1.221924GB  193.1842ms  Host To Device\n",
            "   40040  32.000KB  4.0000KB  60.000KB  1.221924GB  142.9966ms  Device To Host\n",
            "   20021         -         -         -           -   2.180287s  Gpu page fault groups\n",
            "Total CPU Page faults: 40040\n"
          ]
        }
      ]
    }
  ]
}